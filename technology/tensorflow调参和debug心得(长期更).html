<!DOCTYPE HTML>
<html lang="zh-CN">
<head>
<meta charset="utf-8">
<title>深度学习debug心得（长期更）</title>
<meta name="keywords" content="deep_learning_parameter_turning" />


</head>

<body>
    <h1>1关于test_loss出现nan</h1>
    <p>我训练一个cnn，当loss不断下降时，突然test_loss变成了nan，问了一圈，可能的原因是数值溢出，梯度太大，sqrt或log中传入了0或者负数，解决方法1换掉sigmoid激活函数，
    因为里面有log2加正则项让某些梯度减小3降低学习率，因为学习率大很容易减去一个很大的梯度，从而造成负数4激活函数和权重初始化不要在[-1，1]之间5用float32代替float
    64。说实话这些方法都用了仍然没有解决。
    </br>
    我开始到处搜索，找到了一个debug的大杀器tfdbg<a href="https://www.tensorflow.org/versions/master/how_tos/debugger/#frequently_asked_questions" >银色子弹（需要外网）</a>，这是tensorflow
    官方配套的debug工具，使用方法只需在常规模型代码下多三行代码
    </br>
    1引入模块from tensorflow.python import debug as tf_debug
    </br>
    2在sess下sess = tf_debug.LocalCLIDebugWrapperSession(sess)开启调试模式
    </br>
    3加入调试的筛选条件sess.add_tensor_filter("has_inf_or_nan", tf_debug.has_inf_or_nan)
    </br>
    跑模型时会弹出一个界面，输入run -f has_inf_or_nan
    </br>
    这时就开始跑模型，模型中如果有参数出现inf或者nan就会报出来，这个有些慢，要等一等，爆出来后点击第一个含有nan或inf的tensor，可以在上方通过点击查看个中属性
    上方第一个list_tensors是去看所有参数，因为没研究出如何返回所以轻易不要点，说实话这种debug工具我还搞不定，数学不够好，过段时间提升一点再做一下研究。
    </br>
    问题还没解决，又搜索，最终找到了原因，因为损失函数选择了交叉熵，里面有log，所以改进交叉熵如<a href="http://stackoverflow.com/questions/33712178/tensorflow-nan-bug" >别人的经验</a>。
    <h2>这次debug还学到了一些小经验</h2>
    1把训练集大小搞成20，跑个几百个epoches过拟合这些样本，这样可以搞清楚网络有没有问题。
    </br>
    2可以把先把每一步都print出来，如sess.run(w_fc1),看看每一步有没有问题。
    </br>
    3网络的参数命名的时候，根据层命名如name='fc1/w_fc1'，因为这样可以在调参的时候显示的更清楚。
    </p>
    <h1>2dropout经验</h1>
    <p>我之前没有dropout实践的经验只是大概知道其相当于给神经网络弄成了ensemble学习，说实话加了dropout网络特别的慢慎用。原理上从这篇<a href="http://www.cnblogs.com/santian/p/5457412.html" >博客</a></p>可以看出两个
    主要的思想</br>
    1dropout层前后两层的w同步消失。</br>
    2dropout层不需要改变输出的个数。</br>
    从这篇明白<a href="http://www.jianshu.com/p/c9f66bc8f96c" >博客</a>“1/keep_prob” 倍的由来。</br>
    最好的实践例子<a href='https://www.tensorflow.org/get_started/mnist/pros'>官网（需翻墙）</a>。要注意的是除了sess.run(train)要加keep_prob意外，其他sess部分也   
    必须加，其他部分就都加1.0，因为不是训练，只是为了让placeholder的值填上。这个例子也不错<a href='http://blog.csdn.net//chenriwei2/article/details/50615769'>博客</a>
    <h1>3mnist数据下不下来，用这篇<a href='http://blog.csdn.net/github_25679381/article/details/54744076'>博客</a>提供的方法，github数据翻墙下载。</h1>
    <p>以后要用到mnist的例子里</br>
    from tensorflow.examples.tutorials.mnist import input_data </br> 
    mnist =input_data.read_data_sets("MNIST_data/", one_hot=True)</br>
    上面两句需要访问http://yann.lecun.com/exdb/mnist/，这个网址翻墙都访问不了，应该是挂掉了，以后要是修好了，上头两句还能用。</br>
    如果上面不能用，就把上述博客里的github下载一下，把mnist.py和MNIST_data文件夹放到程序的目录下面，并且把上面两行命令改为</br>
    from mnist import read_data_sets</br>
    mnist = read_data_sets('MNIST_data', one_hot=True)</br>
    这样就调用了read_data_sets这个函数，读取MNIST_data里的数据了。</p>
    <h1>4梯度裁剪，用这篇<a href='http://stackoverflow.com/questions/36498127/how-to-effectively-apply-gradient-clipping-in-tensor-flow'>回答</a>提供的方法。</h1>
    <p>梯度裁剪顾名思义就是为了防止梯度太大，给梯度设个阈值，一般分为4步<a href='https://www.tensorflow.org/versions/master/api_docs/python/train/optimizers#processing_gradients_before_applying_them'>官网教程</a></br>
    1定义优化器。optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)</br>
    2计算梯度，大多数时候我都不会专门算梯度，因为.minimize(loss)会自动帮我们
更新梯度，但是看不见梯度，也不知道是否发生了梯度爆炸。gvs = optimizer.compute_gradients(cost)</br>
    3设置裁剪条件。capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]</br>
    4应用梯度裁剪到优化器中去。train_op = optimizer.apply_gradients(capped_gvs)
</p>
    5调参中各个层的w记录下来，可视化希望每个层的W都有一个比较好的分布，不要全都趋于0。




</body>
</html>
